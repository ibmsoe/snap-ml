{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This notebook is used to showcase the capability of SnapML - Spark APIs to accept Spark DataFrames.\n",
    "\n",
    "In Spark, a DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.\n",
    "\n",
    "By supporting Spark Dataframes, SnapML API can accept data, for ML Training, coming from a variety of Data sources & in different data formats.\n",
    "\n",
    "This notebook particularly focusses on reading `parquet` formated file residing in `HDFS` filesystem.\n",
    "\n",
    "Prequisties to run this Notebook are,\n",
    "1. Ensure all the Spark python libraries are exported before this `jupyter notebook` is started,\n",
    "```\n",
    "export PYTHONPATH=/mnt/pai/home/josamuel/spark-2.3.0-bin-hadoop2.7/python/:/mnt/pai/home/josamuel/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip:/mnt/pai/home/josamuel/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip\n",
    "```\n",
    "2. Install `snapml-spark 1.3.0` conda package for SnapML libraries which comes with `PowerAI 1.6.1` release.\n",
    "```\n",
    "conda install snapml-spark\n",
    "```\n",
    "3. Start a Spark cluster and update the Spark Master IP appropriately.\n",
    "4. Make sure Hadoop cluster is configured and make the necessary changes to the below cells with the right Namenode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    " .appName(\"Spark-SnapML\")\\\n",
    " .master(\"spark://sparkMaster:7077\")\\\n",
    " .config(\"spark.jars\", \"/mnt/pai/home/josamuel/anaconda3/envs/py36/snap-ml-spark/lib/snap-ml-spark-v1.3.0-ppc64le.jar\")\\\n",
    " .config(\"spark.default.parallelism\",4)\\\n",
    " .config(\"spark.executor.extraJavaOptions\",\"-XX:MaxDirectMemorySize=8192m\")\\\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sparkMaster:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://sparkMaster:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark-SnapML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fff983f6160>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snap_ml_spark import LogisticRegression as snapml_LogisticRegression\n",
    "from snap_ml_spark.Metrics import accuracy, logisticLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/mnt/pai/home/josamuel/data/criteo.kaggle2014'\n",
    "train_filename = filename + '-train.libsvm'\n",
    "test_filename = filename + '-test.libsvm'\n",
    "\n",
    "n_features_ = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count is 7500\n",
      "Count is 2500\n"
     ]
    }
   ],
   "source": [
    "train_data_df = spark.read.format(\"libsvm\")\\\n",
    "    .option(\"numFeatures\", str(n_features_))\\\n",
    "    .load(train_filename)\\\n",
    "    .repartition(4)\\\n",
    "    .cache()\n",
    "    \n",
    "test_data_df = spark.read.format(\"libsvm\")\\\n",
    "    .option(\"numFeatures\", str(n_features_))\\\n",
    "    .load(test_filename)\\\n",
    "    .repartition(4)\\\n",
    "    .cache()\n",
    "    \n",
    "print(\"Count is \" + str(train_data_df.count()))\n",
    "print(\"Count is \" + str(test_data_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dump the test dataset into HDFS in parquet format\n",
    "\n",
    "Note enable the below cell only for the first time to dump the data into HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_df.write.mode('overwrite').parquet(\"hdfs://hdfs_nameNode/train_data.parquet\") \n",
    "# test_data_df.write.mode('overwrite').parquet(\"hdfs://hdfs_nameNode/test_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SnapML: Parquet Format from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_p_df = spark.read.parquet(\"hdfs://hdfs_nameNode/train_data.parquet\")\n",
    "test_data_p_df = spark.read.parquet(\"hdfs://hdfs_nameNode/test_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create snapML Logistic Regression\n",
    "snapml_regularizer = 10.0\n",
    "if use_gpu:\n",
    "    snapml_lr = snapml_LogisticRegression(max_iter=100, regularizer=snapml_regularizer, \n",
    "                                          verbose=False, dual=True, use_gpu=use_gpu, n_threads=-1)\n",
    "else:\n",
    "    snapml_lr = snapml_LogisticRegression(max_iter=50, regularizer=snapml_regularizer, \n",
    "                                          verbose=False, dual=True, use_gpu=use_gpu, n_threads=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DF time: 16.16 s\n"
     ]
    }
   ],
   "source": [
    "# Fit the model and time it\n",
    "snapml_t0 = time.time()\n",
    "snapml_lr.fit(train_data_p_df)\n",
    "snapml_time = time.time() - snapml_t0\n",
    "print(\"Train DF time: %.2f\" % snapml_time, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snapML  accuracy: 0.7840 , time: 16.16 s\n"
     ]
    }
   ],
   "source": [
    "# Perform inference on test data\n",
    "pred = snapml_lr.predict(test_data_p_df)\n",
    "\n",
    "# Compute accuracy\n",
    "snapml_accuracy  = accuracy(pred)\n",
    "\n",
    "# Print off SnapML  result\n",
    "print('snapML  accuracy: %.4f' %snapml_accuracy, \", time: %.2f\" % snapml_time, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkML LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression as sparkml_LogisticRegression\n",
    "\n",
    "n_examples = train_data_p_df.count()\n",
    "\n",
    "# Create sparkML lib Logistic Regression\n",
    "sparkml_lr = sparkml_LogisticRegression(fitIntercept=False, \n",
    "                                        regParam=snapml_regularizer/n_examples, standardization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train SparkML DF time: 40.42 s\n"
     ]
    }
   ],
   "source": [
    "# Fit the model and time it\n",
    "sparkml_t0 = time.time()\n",
    "sparkml_lr_model = sparkml_lr.fit(train_data_p_df)\n",
    "sparkml_time = time.time() - sparkml_t0\n",
    "print(\"Train SparkML DF time: %.2f\" % sparkml_time, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark ML accuracy = 0.784\n"
     ]
    }
   ],
   "source": [
    "# Perform inference on test data\n",
    "predictions = sparkml_lr_model.transform(test_data_p_df)\n",
    "\n",
    "# Compute accuracy\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "sparkml_accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# Print off Spark result\n",
    "print('Spark ML', evaluator.getMetricName(),'=', sparkml_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; Copyright IBM Corporation 2018"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
